diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/unary.mlir b/stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
@@ -123,8 +123,7 @@
 
 // CHECK-LABEL: @transpose
 func.func @transpose(%arg0: tensor<1x2x3xf32>) -> tensor<3x2x1xf32> {
-  // CHECK: %[[VAR0:.*]] = "tosa.const"() <{value = dense<[2, 1, 0]> : tensor<3xi32>}> : () -> tensor<3xi32>
-  // CHECK: %[[VAR1:.*]] = tosa.transpose %arg0, %[[VAR0]]
+  // CHECK: %[[VAR0:.*]] = tosa.transpose %arg0 {perms = array<i32: 2, 1, 0>}
   %0 = "stablehlo.transpose"(%arg0) {permutation = array<i64: 2, 1, 0>} : (tensor<1x2x3xf32>) -> tensor<3x2x1xf32>
   return %0 : tensor<3x2x1xf32>
 }
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
@@ -458,13 +458,10 @@
     }
 
     auto perms = op.getPermutation();
-    auto type = RankedTensorType::get({static_cast<int64_t>(perms.size())},
-                                      rewriter.getI32Type());
     std::vector<int32_t> perms_int32(perms.begin(), perms.end());
-    auto constOp = rewriter.create<tosa::ConstOp>(
-        op->getLoc(), type, DenseIntElementsAttr::get(type, perms_int32));
-    rewriter.replaceOpWithNewOp<tosa::TransposeOp>(op, op.getType(),
-                                                   op.getOperand(), constOp);
+    rewriter.replaceOpWithNewOp<tosa::TransposeOp>(
+        op, op.getType(), op.getOperand(),
+        rewriter.getDenseI32ArrayAttr(perms_int32));
     return success();
   }
 };
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.cpp b/stablehlo/stablehlo/dialect/ChloOps.cpp
--- stablehlo/stablehlo/dialect/ChloOps.cpp
+++ stablehlo/stablehlo/dialect/ChloOps.cpp
@@ -710,31 +710,6 @@
   return success();
 }
 
-LogicalResult RaggedDotOp::inferReturnTypes(
-    MLIRContext*, std::optional<Location>, ValueRange operands,
-    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
-    SmallVectorImpl<Type>& inferredReturnTypes) {
-  RaggedDotOp::Adaptor op(operands, attributes, properties, regions);
-
-  auto rankedLhsType = cast<RankedTensorType>(op.getLhs().getType());
-  auto rankedRhsType = cast<RankedTensorType>(op.getRhs().getType());
-  auto rankedGroupSizesType =
-      cast<RankedTensorType>(op.getGroupSizes().getType());
-  auto raggedDotDimNums = op.getRaggedDotDimensionNumbers();
-
-  inferredReturnTypes.push_back(RankedTensorType::get(
-      inferRaggedDotOutputDimensions(
-          rankedLhsType, rankedRhsType, rankedGroupSizesType,
-          raggedDotDimNums.getLhsBatchingDimensions(),
-          raggedDotDimNums.getRhsBatchingDimensions(),
-          raggedDotDimNums.getLhsContractingDimensions(),
-          raggedDotDimNums.getRhsContractingDimensions(),
-          raggedDotDimNums.getLhsRaggedDimensions(),
-          raggedDotDimNums.getRhsGroupDimensions()),
-      rankedLhsType.getElementType()));
-  return success();
-}
-
 //===----------------------------------------------------------------------===//
 // TopKOp
 //===----------------------------------------------------------------------===//
diff --ruN a/stablehlo/stablehlo/dialect/ChloOps.td b/stablehlo/stablehlo/dialect/ChloOps.td
--- stablehlo/stablehlo/dialect/ChloOps.td
+++ stablehlo/stablehlo/dialect/ChloOps.td
@@ -856,8 +856,7 @@
   let hasCustomAssemblyFormat = 1;
 }
 
-def CHLO_RaggedDotOp : CHLO_Op<"ragged_dot",
-    [Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>]> {
+def CHLO_RaggedDotOp : CHLO_Op<"ragged_dot", [Pure]> {
   string summary = "Computes a matmul over a single ragged dimension";
 
   string description = [{
diff --ruN a/stablehlo/stablehlo/integrations/c/ChloAttributes.cpp b/stablehlo/stablehlo/integrations/c/ChloAttributes.cpp
--- stablehlo/stablehlo/integrations/c/ChloAttributes.cpp
+++ stablehlo/stablehlo/integrations/c/ChloAttributes.cpp
@@ -66,3 +66,127 @@
   return wrap(mlir::chlo::stringifyComparisonType(
       llvm::cast<mlir::chlo::ComparisonTypeAttr>(unwrap(attr)).getValue()));
 }
+
+//===----------------------------------------------------------------------===//
+// RaggedDotDimensionNumbers
+//===----------------------------------------------------------------------===//
+
+MlirAttribute chloRaggedDotDimensionNumbersGet(
+    MlirContext ctx, intptr_t nLhsBatchingDimensions,
+    const int64_t *lhsBatchingDimensions, intptr_t nRhsBatchingDimensions,
+    const int64_t *rhsBatchingDimensions, intptr_t nLhsContractingDimensions,
+    const int64_t *lhsContractingDimensions, intptr_t nRhsContractingDimensions,
+    const int64_t *rhsContractingDimensions, intptr_t nLhsRaggedDimensions,
+    const int64_t *lhsRaggedDimensions, intptr_t nRhsGroupDimensions,
+    const int64_t *rhsGroupDimensions) {
+  return wrap(mlir::chlo::RaggedDotDimensionNumbersAttr::get(
+      unwrap(ctx),
+      llvm::ArrayRef(lhsBatchingDimensions, nLhsBatchingDimensions),
+      llvm::ArrayRef(rhsBatchingDimensions, nRhsBatchingDimensions),
+      llvm::ArrayRef(lhsContractingDimensions, nLhsContractingDimensions),
+      llvm::ArrayRef(rhsContractingDimensions, nRhsContractingDimensions),
+      llvm::ArrayRef(lhsRaggedDimensions, nLhsRaggedDimensions),
+      llvm::ArrayRef(rhsGroupDimensions, nRhsGroupDimensions)));
+}
+
+bool chloAttributeIsARaggedDotDimensionNumbers(MlirAttribute attr) {
+  return llvm::isa<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr));
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsBatchingDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsBatchingDimensions()[pos];
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsBatchingDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsBatchingDimensions()[pos];
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetLhsContractingDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsContractingDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetLhsContractingDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsContractingDimensions()[pos];
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetRhsContractingDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsContractingDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetRhsContractingDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsContractingDimensions()[pos];
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsRaggedDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getLhsRaggedDimensions()[pos];
+}
+
+intptr_t chloRaggedDotDimensionNumbersGetRhsGroupDimensionsSize(
+    MlirAttribute attr) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsGroupDimensions()
+      .size();
+}
+
+int64_t chloRaggedDotDimensionNumbersGetRhsGroupDimensionsElem(
+    MlirAttribute attr, intptr_t pos) {
+  return llvm::cast<mlir::chlo::RaggedDotDimensionNumbersAttr>(unwrap(attr))
+      .getRhsGroupDimensions()[pos];
+}
+
+//===----------------------------------------------------------------------===//
+// PrecisionAttr
+//===----------------------------------------------------------------------===//
+
+MlirAttribute chloPrecisionAttrGet(MlirContext ctx, MlirStringRef value) {
+  std::optional<mlir::chlo::Precision> precision =
+      mlir::chlo::symbolizePrecision(unwrap(value));
+  if (!precision) llvm::report_fatal_error("Invalid value.");
+  return wrap(mlir::chlo::PrecisionAttr::get(unwrap(ctx), precision.value()));
+}
+
+bool chloAttributeIsAPrecisionAttr(MlirAttribute attr) {
+  return llvm::isa<mlir::chlo::PrecisionAttr>(unwrap(attr));
+}
+
+MlirStringRef chloPrecisionAttrGetValue(MlirAttribute attr) {
+  return wrap(mlir::chlo::stringifyPrecision(
+      llvm::cast<mlir::chlo::PrecisionAttr>(unwrap(attr)).getValue()));
+}
diff --ruN a/stablehlo/stablehlo/integrations/c/ChloAttributes.h b/stablehlo/stablehlo/integrations/c/ChloAttributes.h
--- stablehlo/stablehlo/integrations/c/ChloAttributes.h
+++ stablehlo/stablehlo/integrations/c/ChloAttributes.h
@@ -47,6 +47,70 @@
 MLIR_CAPI_EXPORTED MlirStringRef
 chloComparisonTypeAttrGetValue(MlirAttribute attr);
 
+//===----------------------------------------------------------------------===//
+// RaggedDotDimensionNumbers
+//===----------------------------------------------------------------------===//
+
+MLIR_CAPI_EXPORTED MlirAttribute chloRaggedDotDimensionNumbersGet(
+    MlirContext ctx,                                                        //
+    intptr_t nLhsBatchingDimensions, const int64_t *lhsBatchingDimensions,  //
+    intptr_t nRhsBatchingDimensions, const int64_t *rhsBatchingDimensions,  //
+    intptr_t nLhsContractingDimensions,                                     //
+    const int64_t *lhsContractingDimensions,                                //
+    intptr_t nRhsContractingDimensions,                                     //
+    const int64_t *rhsContractingDimensions,                                //
+    intptr_t nLhsRaggedDimensions,                                          //
+    const int64_t *lhsRaggedDimensions,                                     //
+    intptr_t nRhsGroupDimensions,                                           //
+    const int64_t *rhsGroupDimensions);
+
+MLIR_CAPI_EXPORTED bool chloAttributeIsARaggedDotDimensionNumbers(
+    MlirAttribute attr);
+
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsSize(MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsElem(MlirAttribute attr,
+                                                          intptr_t pos);
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsSize(MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsElem(MlirAttribute attr,
+                                                          intptr_t pos);
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetLhsContractingDimensionsSize(
+    MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetLhsContractingDimensionsElem(MlirAttribute attr,
+                                                             intptr_t pos);
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetRhsContractingDimensionsSize(
+    MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetRhsContractingDimensionsElem(MlirAttribute attr,
+                                                             intptr_t pos);
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsSize(MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsElem(MlirAttribute attr,
+                                                        intptr_t pos);
+MLIR_CAPI_EXPORTED intptr_t
+chloRaggedDotDimensionNumbersGetRhsGroupDimensionsSize(MlirAttribute attr);
+MLIR_CAPI_EXPORTED int64_t
+chloRaggedDotDimensionNumbersGetRhsGroupDimensionsElem(MlirAttribute attr,
+                                                       intptr_t pos);
+
+//===----------------------------------------------------------------------===//
+// PrecisionAttr
+//===----------------------------------------------------------------------===//
+
+MLIR_CAPI_EXPORTED MlirAttribute chloPrecisionAttrGet(MlirContext ctx,
+                                                      MlirStringRef value);
+
+MLIR_CAPI_EXPORTED bool chloAttributeIsAPrecisionAttr(MlirAttribute attr);
+
+MLIR_CAPI_EXPORTED MlirStringRef chloPrecisionAttrGetValue(MlirAttribute attr);
+
 #ifdef __cplusplus
 }
 #endif
diff --ruN a/stablehlo/stablehlo/integrations/python/ChloModule.cpp b/stablehlo/stablehlo/integrations/python/ChloModule.cpp
--- stablehlo/stablehlo/integrations/python/ChloModule.cpp
+++ stablehlo/stablehlo/integrations/python/ChloModule.cpp
@@ -21,6 +21,20 @@
 namespace nb = nanobind;
 
 namespace {
+
+// Returns a vector containing integers extracted from an attribute using the
+// two provided callbacks.
+std::vector<int64_t> attributePropertyVector(
+    MlirAttribute attr, llvm::function_ref<intptr_t(MlirAttribute)> sizeFn,
+    llvm::function_ref<int64_t(MlirAttribute, intptr_t)> getFn) {
+  std::vector<int64_t> result;
+  intptr_t size = sizeFn(attr);
+  result.reserve(size);
+  for (intptr_t i = 0; i < size; ++i) {
+    result.push_back(getFn(attr, i));
+  }
+  return result;
+}
 
 auto toPyString(MlirStringRef mlirStringRef) {
   return nb::str(mlirStringRef.data, mlirStringRef.length);
@@ -79,4 +93,88 @@
       .def_property_readonly("value", [](MlirAttribute self) {
         return toPyString(chloComparisonTypeAttrGetValue(self));
       });
+
+  mlir::python::nanobind_adaptors::mlir_attribute_subclass(
+      m, "RaggedDotDimensionNumbers", chloAttributeIsARaggedDotDimensionNumbers)
+      .def_classmethod(
+          "get",
+          [](nb::object cls, const std::vector<int64_t> &lhsBatchingDims,
+             const std::vector<int64_t> &rhsBatchingDims,
+             const std::vector<int64_t> &lhsContractingDims,
+             const std::vector<int64_t> &rhsContractingDims,
+             const std::vector<int64_t> &lhsRaggedDims,
+             const std::vector<int64_t> &rhsGroupDims, MlirContext ctx) {
+            return cls(chloRaggedDotDimensionNumbersGet(
+                ctx, lhsBatchingDims.size(), lhsBatchingDims.data(),
+                rhsBatchingDims.size(), rhsBatchingDims.data(),
+                lhsContractingDims.size(), lhsContractingDims.data(),
+                rhsContractingDims.size(), rhsContractingDims.data(),
+                lhsRaggedDims.size(), lhsRaggedDims.data(), rhsGroupDims.size(),
+                rhsGroupDims.data()));
+          },
+          nb::arg("cls"), nb::arg("lhs_batching_dimensions"),
+          nb::arg("rhs_batching_dimensions"),
+          nb::arg("lhs_contracting_dimensions"),
+          nb::arg("rhs_contracting_dimensions"),
+          nb::arg("lhs_ragged_dimensions"), nb::arg("rhs_group_dimensions"),
+          nb::arg("context").none() = nb::none(),
+          "Creates a RaggedDotDimensionNumbers attribute with the given "
+          "dimension configuration.")
+      .def_property_readonly(
+          "lhs_batching_dimensions",
+          [](MlirAttribute self) {
+            return attributePropertyVector(
+                self, chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsSize,
+                chloRaggedDotDimensionNumbersGetLhsBatchingDimensionsElem);
+          })
+      .def_property_readonly(
+          "rhs_batching_dimensions",
+          [](MlirAttribute self) {
+            return attributePropertyVector(
+                self, chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsSize,
+                chloRaggedDotDimensionNumbersGetRhsBatchingDimensionsElem);
+          })
+      .def_property_readonly(
+          "lhs_contracting_dimensions",
+          [](MlirAttribute self) {
+            return attributePropertyVector(
+                self,
+                chloRaggedDotDimensionNumbersGetLhsContractingDimensionsSize,
+                chloRaggedDotDimensionNumbersGetLhsContractingDimensionsElem);
+          })
+      .def_property_readonly(
+          "rhs_contracting_dimensions",
+          [](MlirAttribute self) {
+            return attributePropertyVector(
+                self,
+                chloRaggedDotDimensionNumbersGetRhsContractingDimensionsSize,
+                chloRaggedDotDimensionNumbersGetRhsContractingDimensionsElem);
+          })
+      .def_property_readonly(
+          "lhs_ragged_dimensions",
+          [](MlirAttribute self) {
+            return attributePropertyVector(
+                self, chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsSize,
+                chloRaggedDotDimensionNumbersGetLhsRaggedDimensionsElem);
+          })
+      .def_property_readonly("rhs_group_dimensions", [](MlirAttribute self) {
+        return attributePropertyVector(
+            self, chloRaggedDotDimensionNumbersGetRhsGroupDimensionsSize,
+            chloRaggedDotDimensionNumbersGetRhsGroupDimensionsElem);
+      });
+
+  mlir::python::nanobind_adaptors::mlir_attribute_subclass(
+      m, "PrecisionAttr", chloAttributeIsAPrecisionAttr)
+      .def_classmethod(
+          "get",
+          [](nb::object cls, const std::string &value, MlirContext ctx) {
+            return cls(chloPrecisionAttrGet(
+                ctx, mlirStringRefCreate(value.c_str(), value.size())));
+          },
+          nb::arg("cls"), nb::arg("value"),
+          nb::arg("context").none() = nb::none(),
+          "Creates a Precision attribute with the given value.")
+      .def_property_readonly("value", [](MlirAttribute self) {
+        return toPyString(chloPrecisionAttrGetValue(self));
+      });
 }
diff --ruN a/stablehlo/stablehlo/integrations/python/tests/chlo.py b/stablehlo/stablehlo/integrations/python/tests/chlo.py
--- stablehlo/stablehlo/integrations/python/tests/chlo.py
+++ stablehlo/stablehlo/integrations/python/tests/chlo.py
@@ -42,3 +42,30 @@
   assert attr is not None
   assert str(attr) == ("#chlo<comparison_type FLOAT>")
   assert attr.value == "FLOAT"
+
+
+@run
+def test_ragged_dot_dimension_numbers():
+  attr = chlo.RaggedDotDimensionNumbers.get(
+      lhs_batching_dimensions=[0],
+      rhs_batching_dimensions=[1],
+      lhs_contracting_dimensions=[2],
+      rhs_contracting_dimensions=[2],
+      lhs_ragged_dimensions=[1],
+      rhs_group_dimensions=[0],
+  )
+  assert attr is not None
+  assert str(attr) == (
+      "#chlo.ragged_dot<lhs_batching_dimensions = [0], "
+      "rhs_batching_dimensions = [1], "
+      "lhs_contracting_dimensions = [2], "
+      "rhs_contracting_dimensions = [2], >"
+      "lhs_ragged_dimensions = [1], "
+      "rhs_group_dimensions = [0]>"
+  )
+  assert attr.lhs_batching_dimensions == [0]
+  assert attr.rhs_batching_dimensions == [1]
+  assert attr.lhs_contracting_dimensions == [2]
+  assert attr.rhs_contracting_dimensions == [2]
+  assert attr.lhs_ragged_dimensions == [1]
+  assert attr.rhs_group_dimensions == [0]
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -924,6 +924,15 @@
   // CHECK: %[[RES:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<7x2xf32>
   // CHECK: return %[[RES]]
   return %0 : tensor<7x2xf32>
+}
+
+// Can't do anything with the dynamic shape, but shouldn't crash.
+// CHECK-LABEL: @dynamic_pad
+func.func @dynamic_pad(%arg0: tensor<?x2x3xi1>, %arg1: tensor<i1>) -> tensor<?x2x1xi1> {
+  %0 = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
+  // CHECK-NEXT: %[[RES:.+]] = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
+  // CHECK-NEXT: return %[[RES]]
+  return %0 : tensor<?x2x1xi1>
 }
 
 // -----
@@ -1908,6 +1917,19 @@
 
 // -----
 
+// CHECK-LABEL: @side_effecting_custom_call
+func.func @side_effecting_custom_call(%arg0: tensor<0xf32>) -> (tensor<0xf32>, tensor<0xf32>) {
+  // CHECK:      %[[CST:.*]] = stablehlo.constant dense<> : tensor<0xf32>
+  // CHECK-NEXT: %[[CC:.*]] = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = true} : (tensor<0xf32>) -> tensor<0xf32>
+  %0 = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = true} : (tensor<0xf32>) -> tensor<0xf32>
+  // CHECK-NOT:  stablehlo.custom_call{{.*}}has_side_effect = false
+  %1 = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = false} : (tensor<0xf32>) -> tensor<0xf32>
+  // CHECK: return %[[CC]], %[[CST]]
+  return %0, %1 : tensor<0xf32>, tensor<0xf32>
+}
+
+// -----
+
 /////////
 // Generic Shape Ops
 
diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.h b/stablehlo/stablehlo/transforms/optimization/Passes.h
--- stablehlo/stablehlo/transforms/optimization/Passes.h
+++ stablehlo/stablehlo/transforms/optimization/Passes.h
@@ -50,6 +50,13 @@
                                           MLIRContext *context,
                                           bool foldFloat = false,
                                           PatternBenefit benefit = 1);
+
+/// Some workloads in XLA import StableHLO from HLO. Since there are a few
+/// differences in HLO (no implicit captures, lots of tuples, etc.), this
+/// set of patterns brings the imported HLO back to a more canonical form
+/// without applying a full set of graph simplifications.
+void populateStablehloHloImportCanonicalizationPatterns(
+    MLIRContext *context, RewritePatternSet *patterns);
 }  // namespace stablehlo
 }  // namespace mlir
 
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -934,8 +934,12 @@
     auto padVal = op.getPaddingValue();
 
     auto resultTy = cast<RankedTensorType>(op.getType());
-
-    if (cast<ShapedType>(operand.getType()).getNumElements() != 0)
+    auto operandTy = cast<RankedTensorType>(operand.getType());
+
+    if (!operandTy.hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "operand shape is dynamic");
+
+    if (operandTy.getNumElements() != 0)
       return rewriter.notifyMatchFailure(op, "operand is not empty tensor");
 
     if (resultTy.hasStaticShape()) {
@@ -1399,6 +1403,12 @@
       return rewriter.notifyMatchFailure(op, "not stablehlo");
     if (isa<ConstantOp>(op))
       return rewriter.notifyMatchFailure(op, "op is empty constant");
+
+    // Skip ops that have memory effects, similar to XLA's zero extent
+    // simplification, replacing these doesn't save any computation.
+    auto effectInterface = dyn_cast<MemoryEffectOpInterface>(op);
+    if (effectInterface && !effectInterface.hasNoEffect())
+      return rewriter.notifyMatchFailure(op, "op has memory effect");
 
     // If the result is a zero-extent tensor, replace the whole op with an empty
     // constant.
@@ -1528,6 +1538,12 @@
             DynamicReshapeOpIsStatic, DynamicIotaIsStatic>(context);
 }
 
+void populateStablehloHloImportCanonicalizationPatterns(
+    MLIRContext *context, RewritePatternSet *patterns) {
+  patterns->add<TupleIsRepacking, TupleIsUnpacked, WhileOpImplicitCapture>(
+      context);
+}
+
 std::unique_ptr<Pass> createStablehloAggressiveSimplificationPass(
     GreedyRewriteConfig config) {
   return std::make_unique<StablehloAggressiveSimplificationPass>(config);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -411,7 +411,7 @@
 // GetTupleElementOp
 
 // Pattern: get_tuple_element(tuple(X_0, X_1, ...), i) -> X_i
-def : Pat<(StableHLO_GetTupleElementOp (StableHLO_TupleOp:$tuple $operands), $idx),
+def TupleIsUnpacked : Pat<(StableHLO_GetTupleElementOp (StableHLO_TupleOp:$tuple $operands), $idx),
           (GetOperandN $tuple, $idx)>;
 
 ////////

